import torch
import torch_npu
import sys
import time
import math

# å°è¯•å¯¼å…¥è‡ªå®šä¹‰ç®—å­åº“
try:
    import sgl_kernel_npu
except ImportError:
    print("Warning: sgl_kernel_npu not found. Assuming it is loaded via torch.ops")

def run_test():
    # --------------------------------------------------------------------------
    # 1. é…ç½®å‚æ•°
    # --------------------------------------------------------------------------
    # æ¨¡æ‹Ÿ LLaMA-3-8B/70B å¸¸è§çš„å½¢çŠ¶
    # K=4096 (Hidden Size), N=4096 (Intermediate), GroupSize=128
    IN_DIM = 4096   
    OUT_DIM = 4096 
    GROUP_SIZE = 128
    
    # ä½ çš„è‡ªå®šä¹‰ç®—å­ç›®å‰åªæ”¯æŒ batch_size = 1
    BATCH_SIZE = 1 
    
    DEVICE = "npu:0"
    DTYPE = torch.float16
    
    print("=" * 70)
    print(f"Test Configuration:")
    print(f"  Shape      : X=[{BATCH_SIZE}, {IN_DIM}], W=[{IN_DIM}, {OUT_DIM}]")
    print(f"  Group Size : {GROUP_SIZE}")
    print(f"  Dtype      : {DTYPE}")
    print(f"  Compare To : torch_npu.npu_weight_quant_batchmatmul")
    print("=" * 70)

    # --------------------------------------------------------------------------
    # 2. æ•°æ®ç”Ÿæˆ
    # --------------------------------------------------------------------------
    torch.manual_seed(42)
    
    # X: [1, K]
    x = torch.randn((BATCH_SIZE, IN_DIM), dtype=DTYPE, device=DEVICE)
    
    # Scales: [Groups, N]
    num_groups = IN_DIM // GROUP_SIZE
    scales = torch.randn((num_groups, OUT_DIM), dtype=DTYPE, device=DEVICE) * 1.0 / math.sqrt(IN_DIM)
    
    # offsets (Offset): [Groups, N]
    # æ³¨æ„ï¼šè¿™é‡Œçš„ offsets æ˜¯æµ®ç‚¹ç±»å‹çš„ Offsetï¼Œå¯¹åº”å…¬å¼ Y = X * (W + Z) * S ä¸­çš„ Z
    offsets = torch.randint(-8, 8, (num_groups, OUT_DIM), device=DEVICE).to(dtype=DTYPE)

    # Weights: [K, N] åŸå§‹ Int8 æƒé‡ (-8 åˆ° 7)
    # æˆ‘ä»¬ç”Ÿæˆ int32 ä½†é™åˆ¶èŒƒå›´åœ¨ int4 å†…
    weight_unpacked = torch.randint(-8, 8, (IN_DIM, OUT_DIM), dtype=torch.int32, device=DEVICE)

    # --------------------------------------------------------------------------
    # 3. æƒé‡æ‰“åŒ… (Packing)
    # --------------------------------------------------------------------------
    print(">> Packing weights using torch_npu.npu_convert_weight_to_int4pack ...")
    # ä½¿ç”¨åä¸ºå®˜æ–¹ API è¿›è¡Œæ‰“åŒ…ï¼Œç¡®ä¿å†…å­˜å¸ƒå±€ç¬¦åˆ NPU ç¡¬ä»¶è¦æ±‚
    # è¾“å…¥: [K, N] int32, è¾“å‡º: [K, N/8] int32 (å†…éƒ¨æ˜¯ç‰¹æ®Šçš„ NPU æ ¼å¼)
    weight_packed = torch_npu.npu_convert_weight_to_int4pack(weight_unpacked)

    # --------------------------------------------------------------------------
    # 4. è¿è¡Œ NPU åŸç”Ÿç®—å­ (Ground Truth)
    # --------------------------------------------------------------------------
    print(">> Running Native Op (npu_weight_quant_batchmatmul)...")
    
    # Warmup
    for _ in range(10):
        y_ref = torch_npu.npu_weight_quant_batchmatmul(
            x, 
            weight_packed, 
            antiquant_scale=scales, 
            antiquant_offset=offsets, 
            antiquant_group_size=GROUP_SIZE
        )
    
    torch.npu.synchronize()
    start_native = time.time()
    
    # Actual Run
    y_ref = torch_npu.npu_weight_quant_batchmatmul(
        x, 
        weight_packed, 
        antiquant_scale=scales, 
        antiquant_offset=offsets, 
        antiquant_group_size=GROUP_SIZE
    )
    
    torch.npu.synchronize()
    time_native = (time.time() - start_native) * 1000

    # --------------------------------------------------------------------------
    # 5. è¿è¡Œè‡ªå®šä¹‰ AscendC ç®—å­
    # --------------------------------------------------------------------------
    print(">> Running Custom Kernel (gemv_w4a16)...")
    
    # è‡ªå®šä¹‰ç®—å­è¾“å…¥éœ€è¦ flatten çš„ x: [K]
    x_flat = x.view(-1)
    
    # Warmup
    for _ in range(10):
        y_custom = torch.ops.npu.gemv_w4a16(x_flat, weight_packed, scales, offsets)
        
    torch.npu.synchronize()
    start_custom = time.time()
    
    # Actual Run
    y_custom = torch.ops.npu.gemv_w4a16(x_flat, weight_packed, scales, offsets)
    
    torch.npu.synchronize()
    time_custom = (time.time() - start_custom) * 1000

    # --------------------------------------------------------------------------
    # 6. ç»“æœå¯¹æ¯”
    # --------------------------------------------------------------------------
    # å°†è‡ªå®šä¹‰è¾“å‡º reshape å› [1, N] ä»¥ä¾¿å¯¹æ¯”
    y_custom = y_custom.view(1, OUT_DIM)
    
    # è½¬æ¢ä¸º float32 è¿›è¡Œé«˜ç²¾åº¦å¯¹æ¯”
    diff = (y_ref.float() - y_custom.float()).abs()
    max_diff = diff.max().item()
    mean_diff = diff.mean().item()
    
    # æ‰“å°æ€§èƒ½å¯¹æ¯”
    print("-" * 70)
    print(f"Performance Comparison:")
    print(f"  Native Op Time : {time_native:.3f} ms")
    print(f"  Custom Op Time : {time_custom:.3f} ms")
    if time_custom < time_native:
        print(f"  >> Speedup     : {time_native / time_custom:.2f}x ğŸš€")
    else:
        print(f"  >> Slowdown    : {time_native / time_custom:.2f}x")

    # æ‰“å°ç²¾åº¦å¯¹æ¯”
    print("-" * 70)
    print(f"Accuracy Verification:")
    print(f"  Max Diff       : {max_diff:.6f}")
    print(f"  Mean Diff      : {mean_diff:.6f}")
    
    # é˜ˆå€¼åˆ¤å®š
    # BF16 ç²¾åº¦ä¸‹ï¼Œç§¯ç´¯è¯¯å·®å¯èƒ½ä¼šè¾¾åˆ° 1e-2 çº§åˆ«ï¼Œå¯¹äºå¤§çŸ©é˜µä¹˜æ³•æ˜¯æ­£å¸¸çš„
    threshold = 0.05 
    
    if max_diff < threshold or mean_diff < 0.005:
        print("\nâœ… Result Matches! Test PASSED.")
    else:
        print("\nâŒ Result Mismatch! Test FAILED.")
        
        # Debug info
        print("\nDebug First Error:")
        mask = diff > threshold
        indices = torch.nonzero(mask, as_tuple=False)
        if indices.numel() > 0:
            idx = indices[0]
            r, c = idx[0].item(), idx[1].item()
            print(f"  At index [{r}, {c}]:")
            print(f"    Native : {y_ref[r, c].item():.6f}")
            print(f"    Custom : {y_custom[r, c].item():.6f}")
            print(f"    Diff   : {diff[r, c].item():.6f}")

if __name__ == "__main__":
    run_test()